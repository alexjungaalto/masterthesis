\section{Numerical Experiments}
\label{sec_numexp}

This section reports the results of some illustrative numerical experiments to verify the performance 
of Algorithm \ref{alg1}. We provide the code to reproduce these experiments in the supplementary 
material. The experiments discussed in Section \ref{sbm_experiment_section} - Section \ref{stargraph_experiment_section} 
revolve around synthetic datasets whose empirical graph is either a chain graph, a star graph 
or a realization of a Stochastic Block Model (SBM) \cite{AbbeSBM2018}. A numerical experiment with a handwritten digit 
image dataset (``MNIST'') is discussed in Section \ref{mnist_section}. Section \ref{FMI_experiment_section} 
discusses a numerical experiment based on open weather data provided by the Finnish Meteorological Institute (FMI). 

\subsection{Stochastic Block Model} 
\label{sbm_experiment_section}

This experiment revolves around synthetic data whose empirical graph $\graph^{(\rm SBM)}$ is partitioned 
into two equal-sized clusters $\partition = \{ \cluster{1}, \cluster{2}\}$, with $|\cluster{1}| = |\cluster{2}|$. We denote 
the cluster assignment of node $\nodeidx \in \nodes$ by $\clusteridx^{(\nodeidx)} \in \{1,2\}$. The 
edges in $\graph^{(\rm SBM)}$ are generated via realizations of independent binary 
random variables $b_{\nodeidx,\nodeidx'} \in \{0,1\}$. These random variables are indexed by pairs $\nodeidx,\nodeidx'$ of nodes 
that are connected by an edge $\edge{\nodeidx}{\nodeidx'} \in \edges$ if and only if $b_{\nodeidx,\nodeidx'}=1$. Two nodes 
in the same cluster are connected with probability ${\rm Prob}\{ b_{\nodeidx,\nodeidx'}=1 \} \defeq p_{\rm in}$ if $\nodeidx,\nodeidx'$. 
In contrast, ${\rm Prob}\{ b_{\nodeidx,\nodeidx'}=1 \} \defeq p_{\rm out}$ if nodes $\nodeidx,\nodeidx'$ belong to different clusters. 
%nodes $\nodeidx,\nodeidx' \in \cluster{\clusteridx}$ in the same cluster are connected by an edge 
%with probability $p_{\rm in}$. On the other hand, nodes $\nodeidx \in \cluster{1}, \nodeidx' \in \cluster{2}$ 
%n different clusters are connected by an edge with probability $p_{\rm out}$ where typically $p_{\rm out} \ll p_{\rm in}$.
Every edge in $\graph^{(\rm SBM)}$ has the same weight, $\edgeweight_{\edgeidx} = 1$ for all $\edgeidx \in \edges$.  

Each node $\nodeidx \in \nodes$ of the empirical graph $\graph^{(\rm SBM)}$ holds a local dataset $\localdataset{\nodeidx}$ of the 
form \eqref{equ_def_local_dataset_plain}. Thus, the dataset $\localdataset{\nodeidx}$ consists of $\localsamplesize{\nodeidx}$ 
data points, each characterized by a feature vector $ \featurevec^{(\nodeidx,\localsampleidx)} \in \mathbb{R}^{\dimlocalmodel}$ 
and a scalar label $\truelabel^{(\nodeidx,\localsampleidx)}$, for $\localsampleidx=1,\ldots,\localsamplesize{\nodeidx}$. 
The feature vectors $ \featurevec^{(\nodeidx,\sampleidx)} \sim \mathcal{N}(\mathbf{0},\mathbf{I}_{\dimlocalmodel \times \dimlocalmodel})$, 
are drawn i.i.d.\ from a standard multivariate normal distribution. The labels of the data points are generated by a 
noisy linear model 
\begin{equation} 
\label{equ_def_true_linear_model_SBM}
\truelabel^{(\nodeidx,\sampleidx)} = \big( \overline{\weights}^{(\nodeidx)}\big)^{T} \featurevec^{(\nodeidx,\sampleidx)} + \sigma \varepsilon^{(\nodeidx,\sampleidx)}. 
\vspace{-2mm}
\end{equation} 
The noise $\varepsilon^{(\nodeidx,\sampleidx)} \sim  \mathcal{N}(0,1)$, for $\nodeidx \in \nodes$ and $\sampleidx=1,\ldots,\localsamplesize{\nodeidx}$, 
are i.i.d.\ realizations of a standard normal distribution. The true underlying weight vector 
$\overline{\weights}^{(\nodeidx)}$ is piece-wise constant over the clusters in the partition 
$\partition = \{ \cluster{1}, \cluster{2} \}$, i.e., $\overline{\weights}^{(\nodeidx)} = \overline{\weights}^{(\nodeidx')}$ if $\nodeidx,\nodeidx' \in \cluster{\clusteridx}$. 

To study the robustness of Algorithm \ref{alg1} against node failures, we assume that local datasets 
are only accessible in a subset $\trainingset \subseteq \nodes$. The set $\trainingset$ is 
selected uniformly at random among all nodes $\nodes$. We can access local datasets $\localdataset{\nodeidx}$ 
only in the subset $\trainingset$ of relative size $\rho \defeq |\trainingset|/|\nodes|$. The inaccessibility 
of a local dataset $\localdataset{\nodeidx}$, for $\nodeidx \notin \trainingset$, can be modelled by using a trivial loss 
function. Thus, we learn local model parameters $\estlocalparams{\nodeidx}$ using Algorithm \ref{alg1} with 
local loss functions 
\begin{equation} 
	\label{equ_def_local_loss_squared}
	\locallossfunc{\nodeidx}{\localparams{\nodeidx}}  \defeq \begin{cases} 
 \frac{1}{\localsamplesize{\nodeidx}} 
 % (1/\localsamplesize{\nodeidx})
 \sum_{\sampleidx=1}^{\localsamplesize{\nodeidx}} \big( \big(\featurevec^{(\nodeidx,\sampleidx)}\big)^{T} \localparams{\nodeidx} - \truelabel^{(\nodeidx,\sampleidx)}\big)^{2} \mbox{ for } \nodeidx \in \trainingset \\ 
		0 \mbox{  otherwise.}
		\end{cases}. 
  % \vspace{-2mm}
\end{equation} 

The special case $\rho=1$ is obtained when all local datasets are accessible, i.e., when $\trainingset = \nodes$. 
%We used several instances of Algorithm \ref{alg1} obtained from different choices for the GTV penalty functions $\gtvpenalty(\cdot)$. 
For the stopping criterion in Algorithm \ref{alg1} we use a fixed number $\nriter$ of iterations, which 
is $\nriter=3000$ for the results depicted in Figure \ref{fig_noiseless} and $\nriter\!=\!2000$ for 
the results depicted in Figure \ref{fig_fixes_trin_set}. The GTV regularization parameter has 
been set to $\regparam\!=\!10^{-2}$ (see \eqref{equ_def_reguarlizated-local_loss}). We measure 
the estimation error incurred by the learnt parameter vectors $\estlocalparams{\nodeidx}$ using the 
average squared estimation error (MSE), 
\begin{equation}
	\label{equ_def_MSE}
	\estmse \defeq (1/|\nodes|) \sum_{\nodeidx \in \nodes} \|\estlocalparams{\nodeidx}-\overline{\weights}^{(\nodeidx)} \|_{2}^{2}. 
 \vspace{-2mm}
\end{equation} 

\subsubsection{High-Dimensional Linear Regression}
% \url{https://github.com/sahelyiyi/FederatedLearning/blob/chain/experiments/SBM_experiment_ifca.ipynb}

Table \ref{tab:sbm_table} reports the results obtained by applying Algorithm \ref{alg1} to $\graph^{(\rm SBM)}$ with 
$|\cluster{1}| = |\cluster{2}| = 100$, $p_{\rm in}=0.5$, and $p_{\rm out}=10^{-2}$. Each node carries a local dataset 
of the form \eqref{equ_def_local_dataset_plain} with $\localsamplesize{\nodeidx}\!=\!10$ data points, having a feature vector  
$\featurevec^{(\nodeidx,\localsampleidx)} \in \mathbb{R}^{100}$ and labels generated according to \eqref{equ_def_true_linear_model_SBM} 
with noise strength $\sigma=10^{-3}$. The true underlying parameter vectors in \eqref{equ_def_true_linear_model_SBM} 
are cluster-wise constant, $\overline{\weights}^{(\nodeidx)} = \overline{\weights}^{(\clusteridx)}$ for all $\nodeidx \in \cluster{\clusteridx}$. 
The cluster-wise parameter vector $\overline{\weights}^{(\clusteridx)} \in \{0,0.5\}^{100}$ is constructed entry-wise 
using i.i.d.\ realizations of standard Bernoulli variables $B \in \{0, 0.5\}$ with ${\rm Prob}(B=0)=1/2$. 

We learn local model parameters using Algorithm \ref{alg1} using local loss functions \eqref{equ_def_local_dataset_plain} and 
assuming that each local dataset is available, $\trainingset = \nodes$. Table \ref{tab:sbm_table} 
reports the MSE \eqref{equ_def_MSE} incurred by Algorithm \ref{alg1} using a fixed number of $\nriter=1000$ iterations. 
Table \ref{tab:sbm_table} also reports the MSE \eqref{equ_def_MSE} incurred by the model parameters 
learnt by IFCA \cite{Ghosh2020} and FedAvg \cite{Sun2021DecentralizedFA}. 

\begin{table}[htb]
    \centering
    \begin{tabular}{|c|c|} 
    \hline
    Method & MSE \\
    \hline
    Algorithm \ref{alg1} & 1.42e-05 \\ 
    IFCA & 2.82 \\ 
    FedAvg & 2.86 \\ 
    \hline
    \end{tabular}
\vspace*{3mm}
    \caption{MSE \eqref{equ_def_MSE} of the estimation error incurred by Algorithm \ref{alg1}, IFCA \cite{Ghosh2020} and FedAvg \cite{Sun2021DecentralizedFA}}
    \label{tab:sbm_table}
    \vspace{-3mm}
\end{table}

\subsection{Chain Graph} 
\label{chain_section}
% \url{https://github.com/sahelyiyi/FederatedLearning/blob/chain/experiments/chain_experiment.ipynb} 
This experiment uses a dataset with $2 \nrnodes$ local datasets whose empirical graph $\graph^{(\rm chain)}$ is a chain graph. 
The edge set of this graph is given by $\edges = \{ \edge{\nodeidx}{\nodeidx+1} : \nodeidx \in \{1,\ldots,2\nrnodes-1 \} \}$. 
The nodes $\nodes = \{1,\ldots,2 \nrnodes\}$ are partitioned into two clusters $\cluster{1} = \{1,\ldots,\nrnodes\}$ 
and $\cluster{2} = \{\nrnodes+1,\ldots,2 \nrnodes\}$. Every intra-cluster edge $\edge{\nodeidx}{\nodeidx'} \in \edges$, 
with $\nodeidx,\nodeidx'$ in the same cluster, has weight $\edgeweight_{\nodeidx,\nodeidx'} =1$. The single 
inter-cluster edge $\edgeidx' = \{\nrnodes,\nrnodes+1\}$ has weight $\edgeweight_{\nrnodes,\nrnodes+1}  = \varepsilon$ 
with some $\varepsilon \geq 0$. 

The nodes of $\graph^{(\rm chain)}$ carry local datasets with the same structure as in Section \ref{sbm_experiment_section}. 
In particular, each local dataset consists of data points that are characterized by feature vectors drawn from a 
multivariate normal distribution and a label that is generated from \eqref{equ_def_true_linear_model_SBM}. 
The true local parameter vectors (``ground truth'') in \eqref{equ_def_true_linear_model_SBM} are piece-wise 
constant over clusters $\cluster{1}$ and $\cluster{2}$. 

We use Algorithm \ref{alg1} to learn the local parameter vectors in \eqref{equ_def_true_linear_model_SBM} 
using local loss function obtained from the average squared error \eqref{equ_def_local_loss_squared}. 
The local datasets are only accessible for a subset $\samplingset \subseteq \nodes$ of $\lceil \rho |\nodes| \rceil$ 
nodes selected uniformly at random among all nodes $\nodes$. We apply Algorithm \ref{alg1} to the local loss 
functions \eqref{equ_def_local_loss_squared} and using a fixed number $\nriter\!=\!2000$ of iterations and 
different choices for the GTV parameter $\regparam$ as indicated in Figure \ref{fig_noiseless_chain} and Figure \ref{fig_fixes_trin_set_chain}. 

Figure \ref{fig_noiseless_chain} and Figure \ref{fig_fixes_trin_set_chain} depict the MSE \eqref{equ_def_MSE} incurred 
by Algorithm \ref{alg1} obtained for varying training size $\rho$, noise strength $\sigma$ (see \eqref{equ_def_true_linear_model_SBM}) 
and inter-cluster edge weight $\varepsilon$. The curves and bars in Figures \ref{fig_noiseless_chain} and \ref{fig_fixes_trin_set_chain} 
represent the average and standard deviation of MSE values of $5$ simulation runs for different choices for $\rho, \varepsilon$ and $\sigma$. 
Figure \ref{fig_noiseless_chain} shows the results in the noise-less case where $\sigma=0$ in \eqref{equ_def_true_linear_model_SBM}, 
and Figure \ref{fig_fixes_trin_set_chain} shows results for varying noise level $\sigma$ in \eqref{equ_def_true_linear_model_SBM} 
and fixed relative training size $\rho=6/10$. 


\begin{figure}[htbp]
\begin{minipage}[t]{0.3\columnwidth} 
%%Plot of different sampling ratios for norm1
\begin{tikzpicture}[scale=0.5]
\begin{axis}[
ymode=log, ymin=1e-11, ymax=1e0,
y label style={at={(axis description cs:0.040,.5)},rotate=0.0,anchor=south},
label style={font=\Large},
title style={font=\Large},
legend style={font=\Large},
yticklabel style = {font=\small},
xticklabel style = {font=\Large},
xlabel={$\varepsilon$},
ylabel={MSE},
title={$\phi(\vu)\!=\!\|\vu\|_{1},\regparam\!=\!0.1$},
legend pos=south east,
ymajorgrids=true,
grid style=dashed,
]

\addplot+[color=cyan!40!blue!60, very thick, solid, mark=*, error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=epsilon, y=N1M02MSE, y error=N1M02STD, col sep=comma]{differentSamplingRatiosEpsilon.csv};\addlegendentry{$\rho\!=\!2/10$}
\addplot+[color=orange, very thick, densely dotted, mark=*, error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=epsilon, y=N1M04MSE, y error=N1M04STD, col sep=comma]{differentSamplingRatiosEpsilon.csv};\addlegendentry{$\rho\!=\!4/10$}
\addplot+[color=green!40!teal!60, very thick, densely dashed, mark=*, error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=epsilon, y=N1M06MSE, y error=N1M06STD, col sep=comma]{differentSamplingRatiosEpsilon.csv};\addlegendentry{$\rho\!=\!6/10$}
%\addplot[color=cyan!40!blue!60, very thick, solid, mark=*] table [x=epsilon, y=N1M02MSE, col sep=comma]{differentSamplingRatiosEpsilon.csv};\addlegendentry{$\rho\!=\!0.2$}
%\addplot[color=orange, very thick, densely dotted, mark=*] table [x=epsilon, y=N1M04MSE, col sep=comma]{differentSamplingRatiosEpsilon.csv};\addlegendentry{$\rho\!=\!0.4$}
%\addplot[color=green!40!teal!60, very thick, densely dashed, mark=*] table [x=epsilon, y=N1M06MSE, col sep=comma]{differentSamplingRatiosEpsilon.csv};\addlegendentry{$\rho\!=\!0.6$}
\end{axis}
\end{tikzpicture}
\end{minipage}
\hspace*{15mm}
\begin{minipage}[t]{0.3\columnwidth} 
%%Plot of different sampling ratios for norm2
\begin{tikzpicture}[scale=0.5]
\begin{axis}[
ymode=log, ymin=1e-11, ymax=1e0,
y label style={at={(axis description cs:0.040,.5)},rotate=0.0,anchor=south},
label style={font=\Large},
title style={font=\Large},
legend style={font=\Large},
yticklabel style = {font=\small},
xticklabel style = {font=\Large},
xlabel={$\varepsilon$},
ylabel={MSE},
title={$\phi(\vu)\!=\!\|\vu\|_{2},\regparam\!=\!0.1$ (nLasso)},
legend pos=south east,
ymajorgrids=true,
grid style=dashed,
]
\addplot+[color=cyan!40!blue!60, very thick, solid, mark=*, error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=epsilon, y=N2M02MSE, y error=N2M02STD, col sep=comma]{differentSamplingRatiosEpsilon.csv};\addlegendentry{$\rho\!=\!2/10$}
\addplot+[color=orange, very thick, densely dotted, mark=*, error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=epsilon, y=N2M04MSE, y error=N2M04STD, col sep=comma]{differentSamplingRatiosEpsilon.csv};\addlegendentry{$\rho\!=\!4/10$}
\addplot+[color=green!40!teal!60, very thick, densely dashed, mark=*, error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=epsilon, y=N2M06MSE, y error=N2M06STD, col sep=comma]{differentSamplingRatiosEpsilon.csv};\addlegendentry{$\rho\!=\!6/10$}
\end{axis}
\end{tikzpicture}
\end{minipage}
\\
\\
\hspace*{22mm}
\begin{minipage}[t]{0.3\columnwidth} 
%%Plot of different sampling ratios for mocha
\begin{tikzpicture}[scale=0.5]
\begin{axis}[
ymode=log, ymin=1e-11, ymax=1e0,
y label style={at={(axis description cs:0.040,.5)},rotate=0.0,anchor=south},
label style={font=\Large},
title style={font=\Large},
legend style={font=\Large},
yticklabel style = {font=\small},
xticklabel style = {font=\Large},
xlabel={$\varepsilon$},
ylabel={MSE},
title={$\phi(\vu)\!=\!\|\vu\|_{2}^{2},\regparam\!=\!0.05$ (``MOCHA'')},
legend pos=south east,
ymajorgrids=true,
grid style=dashed,
]

%\addplot[color=cyan!40!blue!60, very thick, solid, mark=*] table [x=epsilon, y=MCM02MSE, col sep=comma]{differentSamplingRatiosEpsilon.csv};\addlegendentry{$\rho\!=\!0.2$}
%\addplot[color=orange, very thick, densely dotted, mark=*] table [x=epsilon, y=MCM04MSE, col sep=comma]{differentSamplingRatiosEpsilon.csv};\addlegendentry{$\rho\!=\!0.4$}
%\addplot[color=green!40!teal!60, very thick, densely dashed, mark=*] table [x=epsilon, y=MCM06MSE, col sep=comma]{differentSamplingRatiosEpsilon.csv};\addlegendentry{$\rho\!=\!0.6$}

\addplot+[color=cyan!40!blue!60, very thick, solid, mark=*, error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=epsilon, y=MCM02MSE, y error=MCM02STD, col sep=comma]{differentSamplingRatiosEpsilon.csv};\addlegendentry{$\rho\!=\!0.2$}
\addplot+[color=orange, very thick, densely dotted, mark=*, error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=epsilon, y=MCM04MSE, y error=MCM04STD, col sep=comma]{differentSamplingRatiosEpsilon.csv};\addlegendentry{$\rho\!=\!0.4$}
\addplot+[color=green!40!teal!60, very thick, densely dashed, mark=*, error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=epsilon, y=MCM06MSE, y error=MCM06STD, col sep=comma]{differentSamplingRatiosEpsilon.csv};\addlegendentry{$\rho\!=\!0.6$}

\end{axis}
\end{tikzpicture}
\end{minipage}
\caption{\label{fig_noiseless_chain} MSE \eqref{equ_def_MSE} of the estimation error incurred by 
	Algorithm \ref{alg1} when learning local model parameters for a chain graph $\graph^{(\rm chain)}$ in the 
	noiseless case $\sigma=0$.} %when 
%$\rho=|\trainingset|/|\nodes|$ and inter-cluster probability $p_{\rm out}$. The intra cluster probability and noise strength 
%is fixed to $p_{\rm in}=1/2???$ and $\sigma = 0???$, respectively. Each plot depicts average and standard deviation of MSE values obtained from 
%$N=???$ simulation runs.}
%}
\vspace{-3mm}
\end{figure}

\begin{figure}[htbp]
\begin{minipage}[t]{0.3\columnwidth} 
%%Plot of different noises for norm1
\begin{tikzpicture}[scale=0.5]
\begin{axis}[
ymode=log, ymin=1e-7, ymax=1e0,
y label style={at={(axis description cs:0.040,.5)},rotate=0.0,anchor=south},
label style={font=\Large},
title style={font=\Large},
legend style={font=\Large},
yticklabel style = {font=\small},
xticklabel style = {font=\Large},
xlabel={$\epsilon$},
ylabel={MSE},
title={$\phi(\vu)\!=\!\|\vu\|_{1},\regparam\!=\!0.1$},
legend pos=south east,
ymajorgrids=true,
grid style=dashed,
]

%\addplot[color=cyan!40!blue!60, very thick, solid, mark=*] table [x=epsilon, y=N1N001MSE, col sep=comma]{differentNoisesEpsilon.csv};\addlegendentry{$\sigma=0.01$}
%\addplot[color=orange, very thick, densely dotted, mark=*] table [x=epsilon, y=N1N01MSE, col sep=comma]{differentNoisesEpsilon.csv};\addlegendentry{$\sigma=0.1$}
%\addplot[color=green!40!teal!60, very thick, densely dashed, mark=*] table [x=epsilon, y=N1N10MSE, col sep=comma]{differentNoisesEpsilon.csv};\addlegendentry{$\sigma=1.0$}

\addplot+[color=cyan!40!blue!60, very thick, solid, mark=*, error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=epsilon, y=N1N001MSE, y error=N1N001STD, col sep=comma]{differentNoisesEpsilon.csv};\addlegendentry{$\sigma\!=\!10^{-2}$}
\addplot+[color=orange, very thick, densely dotted, mark=*, error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=epsilon, y=N1N01MSE, y error=N1N01STD, col sep=comma]{differentNoisesEpsilon.csv};\addlegendentry{$\sigma\!=\!10^{-1}$}
\addplot+[color=green!40!teal!60, very thick, densely dashed, mark=*, error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=epsilon, y=N1N10MSE, y error=N1N10STD, col sep=comma]{differentNoisesEpsilon.csv};\addlegendentry{$\sigma\!=\!1$}


\end{axis}
\end{tikzpicture}
\end{minipage}
\hspace*{15mm}
\begin{minipage}[t]{0.3\columnwidth} 
%%Plot of different noises for norm2
\begin{tikzpicture}[scale=0.5]
\begin{axis}[
ymode=log, ymin=1e-7, ymax=1e0,
y label style={at={(axis description cs:0.040,.5)},rotate=0.0,anchor=south},
label style={font=\Large},
title style={font=\Large},
legend style={font=\Large},
yticklabel style = {font=\small},
xticklabel style = {font=\Large},
xlabel={$\varepsilon$},
ylabel={MSE},
title={$\gtvpenalty(\vu)\!=\!\|\vu\|_{2},\regparam\!=\!0.1$ (nLasso)},
legend pos=south east,
ymajorgrids=true,
grid style=dashed,
]

%\addplot[color=cyan!40!blue!60, very thick, solid, mark=*] table [x=epsilon, y=N2N001MSE, col sep=comma]{differentNoisesEpsilon.csv};\addlegendentry{$\sigma\!=\!0.01$}
%\addplot[color=orange, very thick, densely dotted, mark=*] table [x=epsilon, y=N2N01MSE, col sep=comma]{differentNoisesEpsilon.csv};\addlegendentry{$\sigma\!=\!0.1$}
%\addplot[color=green!40!teal!60, very thick, densely dashed, mark=*] table [x=epsilon, y=N2N10MSE, col sep=comma]{differentNoisesEpsilon.csv};\addlegendentry{$\sigma\!=\!1.0$}

\addplot+[color=cyan!40!blue!60, very thick, solid, mark=*, error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=epsilon, y=N2N001MSE, y error=N2N001STD, col sep=comma]{differentNoisesEpsilon.csv};\addlegendentry{$\sigma\!=\!10^{-2}$}
\addplot+[color=orange, very thick, densely dotted, mark=*, error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=epsilon, y=N2N01MSE, y error=N2N01STD, col sep=comma]{differentNoisesEpsilon.csv};\addlegendentry{$\sigma\!=\!10^{-1}$}
\addplot+[color=green!40!teal!60, very thick, densely dashed, mark=*, error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=epsilon, y=N2N10MSE, y error=N2N10STD, col sep=comma]{differentNoisesEpsilon.csv};\addlegendentry{$\sigma\!=\!1$}

\end{axis}
\end{tikzpicture}
\end{minipage}
\\
\\
\hspace*{22mm} 
\begin{minipage}[t]{0.3\columnwidth} 
%%Plot of different noises for mocha
\begin{tikzpicture}[scale=0.5]
\begin{axis}[
ymode=log, ymin=1e-7, ymax=1e0,
y label style={at={(axis description cs:0.040,.5)},rotate=0.0,anchor=south},
label style={font=\Large},
title style={font=\Large},
legend style={font=\Large},
yticklabel style = {font=\small},
xticklabel style = {font=\Large},
xlabel={$\varepsilon$},
ylabel={MSE},
title={$\gtvpenalty(\vu)\!=\!\|\vu\|^2_{2},\regparam\!=\!0.05$ (``MOCHA'')},
legend pos=south east,
ymajorgrids=true,
grid style=dashed,
]

%\addplot[color=cyan!40!blue!60, very thick, solid, mark=*] table [x=epsilon, y=MCN001MSE, col sep=comma]{differentNoisesEpsilon.csv};\addlegendentry{$\sigma\!=\!0.01$}
%\addplot[color=orange, very thick, densely dotted, mark=*] table [x=epsilon, y=MCN01MSE, col sep=comma]{differentNoisesEpsilon.csv};\addlegendentry{$\sigma\!=\!0.1$}
%\addplot[color=green!40!teal!60, very thick, densely dashed, mark=*] table [x=epsilon, y=MCN10MSE, col sep=comma]{differentNoisesEpsilon.csv};\addlegendentry{$\sigma\!=\!1.0$}

\addplot+[color=cyan!40!blue!60, very thick, solid, mark=*, error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=epsilon, y=MCN001MSE, y error=MCN001STD, col sep=comma]{differentNoisesEpsilon.csv};\addlegendentry{$\sigma\!=\!10^{-2}$}
\addplot+[color=orange, very thick, densely dotted, mark=*, error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=epsilon, y=MCN01MSE, y error=MCN01STD, col sep=comma]{differentNoisesEpsilon.csv};\addlegendentry{$\sigma\!=\!10^{-1}$}
\addplot+[color=green!40!teal!60, very thick, densely dashed, mark=*, error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=epsilon, y=MCN10MSE, y error=MCN10STD, col sep=comma]{differentNoisesEpsilon.csv};\addlegendentry{$\sigma\!=\!1$}
\end{axis}
\end{tikzpicture}
\end{minipage}
\caption{\label{fig_fixes_trin_set_chain} $\estmse$ \eqref{equ_def_MSE} incurred by Algorithm \ref{alg1} when learning 
	the local model parameters for the chain graph $\graph^{(\rm chain)}$ using only local datasets at a fraction of 
	$|\trainingset|/|\nodes|=0.6$ nodes (see \eqref{equ_def_local_loss_squared}).} 
 \vspace{-3mm}
\end{figure}

%\newpage
\subsection{Handwritten Digits} 
\label{mnist_section}
% \url{https://github.com/sahelyiyi/FederatedLearning/blob/chain/experiments/mnist_NN_experiment.ipynb}

This experiment revolves around a collection of $\nrnodes=40$ local datasets generated from the handwritten image 
dataset (``MNIST dataset'') \cite{LeCun1998}. and represented by the empirical graph $\graphmnist = \pair{\nodesmnist}{\edgesmnist}$. 
Each node $\nodeidx \in \nodesmnist=\{1,\ldots,40\}$ carries a local dataset $\localdataset{\nodeidx}$ which 
consists of $\localsamplesize{\nodeidx}=500$ data points being images of handwritten digits. 
The $\sampleidx$-th data point in $\localdataset{\nodeidx}$ is characterized by the feature vector $\featurevec^{(\nodeidx,\sampleidx)} \in \mathbb{R}^{\dimlocalmodel}$ and a label that specifies the digit that the datapoint belongs to.
% $\truelabel^{(\nodeidx,\sampleidx)} \in \{0,1,\ldots,9\}$ which is the depicted digit. 
The entries of  
the feature vector $\featurevec^{(\nodeidx,\sampleidx)} \in \mathbb{R}^{\dimlocalmodel}$ 
are greyscale levels of $\dimlocalmodel = 28 \times 28$ pixels. 
The nodes $\nodesmnist$ are partitioned into two clusters $\cluster{1},\cluster{2}$ (see Assumption \ref{asspt_weights_clustered}), 
each with $20$ nodes.  The nodes in $\cluster{1}$ carry local datasets that consist of images depicting 
handwritten digits $0$ and $1$. The nodes $\nodeidx \in \cluster{2}$ in the second cluster carry local datasets $\localdataset{\nodeidx}$ 
consisting of images that depict handwritten digits $2$ and $3$. 
%Each node of the empirical graph stands for a simple neural network with two Linear layers in which the dimension of the first layer's input is 28*28 and its% output is 1 (with Relu activation function), and also the last layer has the output with dimension 2 (and Softmax activation function).

Besides the local dataset $\localdataset{\nodeidx}$, the node $\nodeidx \in \nodes$ is also 
assigned a local model in the form of an artificial neural network (ANN),
\begin{equation} 
\label{equ_def_local_ANN_MNIST}
\begin{aligned}
    h^{(\nodeidx)} \big(\featurevec; \localparams{\nodeidx}\big) \defeq {\rm SoftMax}\big( \mW^{(\nodeidx,2)} {\rm ReLU} \big( \mW^{(\nodeidx,1)} \featurevec \big) \big) \\
    \mbox{ with } \localparams{\nodeidx} = {\rm stack} \{\mW^{(\nodeidx,1)} ,\mW^{(\nodeidx,2)}  \}.
\end{aligned}
\vspace{-2mm}
\end{equation} 
The ANN \eqref{equ_def_local_ANN_MNIST} consists of two densely connected layers, with 
the first layer using rectified linear units {\rm ReLU} \cite{Goodfellow-et-al-2016}. The second 
(output) layer uses a ``soft-max'' activation function \cite{Goodfellow-et-al-2016}. 
The weights of connections between layers are stored in the matrices $\mW^{(\nodeidx,1)}$ and $\mW^{(\nodeidx,2)}$, 
respectively. The entries of the weight matrices are collected in the local model parameter vector $\localparams{\nodeidx}$. 

For learning the local model parameters $\localparams{\nodeidx}$ of the ANNs \eqref{equ_def_local_ANN_MNIST}, 
we split each local dataset a training and validation set, $\localdataset{\nodeidx} = \localdatasettrain{\nodeidx} \cup \localdatasetval{\nodeidx}$ 
of size $\localtrainsetsize{\nodeidx}\!=\!400$ and $\localvalsetsize{\nodeidx}\!=\!100$, respectively. 
The local training sets $\localdatasettrain{\nodeidx}$ are also used for the construction of the 
edges in $\graph^{(\rm MNIST)}$ as described next. 

%To construct the edges and their weights for the empirical graph $\graph^{(\rm MNIST)}$ we compute the 
%Kullback-Leibler (KL) divergence between embeddings of the training sets $\localdatasettrain{\localdataset}$. 
%approach. 
For each $\nodeidx \in \nodesmnist$, we apply {\rm t-SNE} \cite{tSNEPaper} to map the raw feature vector 
$\featurevec^{(\nodeidx,\sampleidx)}$ to the embedding vector $\vz^{(\nodeidx,\sampleidx)} \in \mathbb{R}^{2}$, 
for $\sampleidx =1,\ldots, \localtrainsetsize{\nodeidx}$. We then compute a distance  
${\rm dist}(\nodeidx,\nodeidx')$ between any two different nodes $\nodeidx, \nodeidx' \in \nodesmnist$ 
using a Kullback-Leibler divergence estimator \cite{PerezCruz2008}. 

Given the pairwise distances ${\rm dist}(\nodeidx,\nodeidx')$, for any $\nodeidx,\nodeidx' \in \nodesmnist$, 
we construct the edges of $\edgesmnist$ and their weights as follows. Each node $\nodeidx \in \nodesmnist$ 
is connected with the four other nodes $\nodeidx' \in \nodesmnist \setminus \{ \nodeidx\}$ of 
minimum distance ${\rm dist}(\nodeidx,\nodeidx')$ by an edge $\edgeidx=\edge{\nodeidx}{\nodeidx'}$. 
The edge weights are then constructed by an exponential kernel \cite{Luxburg2007}, $\edgeweight_{\nodeidx,\nodeidx'} = {\rm exp} \big(-{\rm dist}(\nodeidx,\nodeidx') \big)$.
% It means that the total number of edges in the graph is $3\times N$ and the degree of each node is exactly $3$.

To learn the local parameters $\estlocalparams{\nodeidx}$ of \eqref{equ_def_local_ANN_MNIST}, we use 
Algorithm \ref{alg1} with local loss 
\begin{equation*}
    \begin{aligned}
        \locallossfunc{\nodeidx}{\localparams{\nodeidx}} & \defeq 
         -\big(1/\localtrainsetsize{\nodeidx}\big)
        \sum_{\left(\featurevec,\truelabel\right) \in \localdatasettrain{\nodeidx}} \big( \truelabel \log{h^{(\nodeidx)}\big(\featurevec\big)} \big ) \\
        - & \big(1/\localtrainsetsize{\nodeidx}\big)  \sum_{\left(\featurevec,\truelabel\right) \in \localdatasettrain{\nodeidx}} \big( (1\!-\!\truelabel) \log{\big(1 - h^{(\nodeidx)}\big(\featurevec\big)\big)} \big ).
    \end{aligned}
    \vspace{-2mm}
\end{equation*}
% \begin{equation*}
%     \begin{aligned}
%         \locallossfunc{\nodeidx}{\localparams{\nodeidx}} & \defeq 
%          -\big(1/\localtrainsetsize{\nodeidx}\big)
%         \sum_{\left(\featurevec,\truelabel\right) \in \localdatasettrain{\nodeidx}} \sum_{j \in \cluster{}} \big( \truelabel_j \log{h^{(\nodeidx)}_j\big(\featurevec\big)} \big ) \\
%     \end{aligned}
% \end{equation*}

% $$ 
% % \big(\featurevec^{(\nodeidx,\sampleidx)}\big)^{T} \localparams{\nodeidx} - \truelabel^{(\nodeidx,\sampleidx)}\big)^{2}
% $$ 
As stopping criterion in Algorithm \ref{alg1}, we use a fixed number of $\nriter=50$ iterations. The GTV parameter 
has been set to $\regparam=1.0$. We measure the quality of the model parameters learnt by Algorithm \ref{alg1} 
via the accuracy ${\rm ACC}^{(\rm val)}$ achieved on the validation sets $\localdatasetval{\nodeidx}$. 
More precisely, ${\rm ACC}^{(\rm val)}$ is the fraction of correctly classified images in the 
validation sets $\localdatasetval{\nodeidx}$, for $\nodeidx \in \nodesmnist$. 
%\begin{equation}
%   {\rm accuracy} \defeq \frac{{\rm number\ of\  correct\ image\ classifications}}{{\rm total\ number\ of\ images}} 
%\end{equation}
%???for all the nodes.????

% Table \ref{tab:mnist_table} depicts the results for the Algorithm \ref{alg1} compared with a case in 
% which all the training images for both of the clusters are aggregated together and a similar 
% simple neural network is used that the last layer has four output neurons (instead of two neurons). 
% As you can see, Algorithm \ref{alg1} boosted the test accuracy of the MNIST dataset.
% \begin{table}[htb]
%     \centering
%     \begin{tabular}{|c|c|c|} 
%     \hline
%     Method & train accuracy & test accuracy \\
%     \hline
%     Algorithm \ref{alg1} & 99\% & 97\% \\ 
%     Aggregated & 97\%  & 64\% \\ 
%     \hline
%     \end{tabular}
% \vspace*{2mm}
%     \caption{Accuracy of the local model parameters learnt by Algorithm \ref{alg1} with $\regparam=1.0$ 
%     	compared with the aggregated dataset (which is equivalent to using Algorithm \ref{alg1} with $\regparam \rightarrow \infty$.}
%     \label{tab:mnist_table}
% \end{table}
Figure \ref{fig:lambda_mnist} shows the validation set accuracy achieved by the local model parameters learnt by 
Algorithm \ref{alg1} for different choices of $\regparam$. For the extreme case $\regparam=0$, Algorithm \ref{alg1} 
ignores the edges in $\graph^{(\rm MNIST)}$ and separately minimizes the local loss functions. The other extreme 
case is $\regparam \rightarrow \infty$ where all local model parameter are enforced to be identical, which is equivalent 
to learning a single model on all pooled local datasets. Figure \ref{fig:coonvergence_mnist} compares the ${\rm ACC}^{(\rm val)}$ 
and the convergence rate of the Algorithm \ref{alg1} with existing methods for personalized FL and for 
clustered FL. Both the ${\rm ACC}^{(\rm val)}$ and the convergence rate of the MNIST dataset is 
improved by Algorithm \ref{alg1}.

\begin{figure}
	\begin{center}
\begin{tikzpicture}[scale=0.5]
\begin{axis}[
% ymode=log,
y label style={at={(axis description cs:0.040,.5)},rotate=0.0,anchor=south},
label style={font=},
title style={font=},
legend style={font=},
yticklabel style = {font=},
xticklabel style = {font=},
xlabel={$\iteridx$},
ylabel={${\rm ACC}^{(\rm val)}$},
title={$\gtvpenalty(\flowvec)\!=\!\|\flowvec\|_{2}$ (nLasso)},
legend pos=south east,
ymajorgrids=true,
grid style=dashed,
]
\addplot+[color=cyan!40!blue!60, very thick, solid, mark=., error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=iter, y=0.0, col sep=comma]{lambda_mnist_journal.csv};\addlegendentry{$\regparam\!=\!0$}
\addplot+[color=orange, very thick, solid, mark=., error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=iter, y=1.0, col sep=comma]{lambda_mnist_journal.csv};\addlegendentry{$\regparam\!=\!1$}
\addplot+[color=green!40!teal!60, very thick, solid, mark=., error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=iter, y=10000.0, col sep=comma]{lambda_mnist_journal.csv};\addlegendentry{$\regparam\!=\!10^{3}$}
\end{axis}
\end{tikzpicture}
\vspace*{-3mm}
\end{center}
\caption{The validation accuracy ${\rm ACC}^{(\rm val)}$ as a function ot the number $\iteridx$ 
	of iterations run by Algorithm \ref{alg1}. Different curves correspond to difference choices for 
	the GTV parameter $\regparam$. \label{fig:lambda_mnist}}
 \vspace{-3mm}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\begin{tikzpicture}[scale=0.5]
\begin{axis}[
% ymode=log,
y label style={at={(axis description cs:0.040,.5)},rotate=0.0,anchor=south},
label style={font=},
title style={font=},
legend style={font=},
yticklabel style = {font=},
xticklabel style = {font=},
xlabel={$\iteridx$},
ylabel={${\rm ACC}^{(\rm val)}$ },
title={$\regparam=1, \gtvpenalty(\flowvec)\!=\!\|\flowvec\|_{2}$ (nLasso)},
legend pos=south east,
ymajorgrids=true,
grid style=dashed,
]
\addplot+[color=cyan!40!blue!60, very thick, solid, mark=., error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=iter, y=alg1, col sep=comma]{compares_mnist_journal.csv};\addlegendentry{Algorithm \ref{alg1}}
\addplot+[color=orange, very thick, solid, mark=., error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=iter, y=ifca, col sep=comma]{compares_mnist_journal.csv};\addlegendentry{IFCA \cite{Ghosh2020}}
\addplot+[color=green!40!teal!60, very thick, solid, mark=., error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=iter, y=pfedme, col sep=comma]{compares_mnist_journal.csv};\addlegendentry{pFedMe \cite{t2020personalized}}
\addplot+[color=purple!40!red!60, very thick, solid, mark=., error bars/.cd, y dir=both, y explicit, error bar style={line width=1pt,solid}, error mark options={line width=1pt,mark size=2pt,rotate=90}] table [x=iter, y=pytorch, col sep=comma]{compares_mnist_journal.csv};\addlegendentry{PyTorch optimizer}
\end{axis}
\end{tikzpicture}
\vspace*{-4mm}
\end{center}
\caption{Comparing the validation set accuracy ${\rm ACC}^{(\rm val)}$ achieved by local model parameters learnt by 
	Algorithm \ref{alg1} with those learnt by IFCA \cite{Ghosh2020}, pFedMe \cite{t2020personalized} and the PyTorch 
	optimizer for \eqref{equ_gtvmin}.}
\label{fig:coonvergence_mnist}
\vspace{-3mm}
\end{figure}

%\newpage


